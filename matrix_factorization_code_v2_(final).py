# -*- coding: utf-8 -*-
"""matrix_factorization_code_v2_(final).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yEpQEULU_pkY8ruIjQtPFlLX884fllLB
"""

import sys
import os

# Get the absolute path of the directory containing utils.py
# Assuming "sample_data" is in the same directory as your current script
utils_dir = os.path.join(os.getcwd(), 'sample_data')

# Add the directory to sys.path
sys.path.append(utils_dir)

# Now you should be able to import from utils
from utils import *
from scipy.linalg import sqrtm

import numpy as np
from utils import load_train_sparse, load_train_csv, load_valid_csv, load_public_test_csv

def svd_reconstruct(matrix, k):
    """ Given the matrix, perform singular value decomposition
    to reconstruct the matrix.

    :param matrix: 2D sparse matrix
    :param k: int
    :return: 2D matrix
    """
    # First, you need to fill in the missing values (NaN) to perform SVD.
    # Fill in the missing values using the average on the current item.
    # Note that there are many options to do fill in the
    # missing values (e.g. fill with 0).
    new_matrix = matrix.copy()
    mask = np.isnan(new_matrix)
    masked_matrix = np.ma.masked_array(new_matrix, mask)
    item_means = np.mean(masked_matrix, axis=0)
    new_matrix = masked_matrix.filled(item_means)

    # Next, compute the average and subtract it.
    item_means = np.mean(new_matrix, axis=0)
    mu = np.tile(item_means, (new_matrix.shape[0], 1))
    new_matrix = new_matrix - mu

    # Perform SVD.
    Q, s, Ut = np.linalg.svd(new_matrix, full_matrices=False)
    s = np.diag(s)

    # Choose top k eigenvalues.
    s = s[0:k, 0:k]
    Q = Q[:, 0:k]
    Ut = Ut[0:k, :]
    s_root = sqrtm(s)

    # Reconstruct the matrix.
    reconst_matrix = np.dot(np.dot(Q, s_root), np.dot(s_root, Ut))
    reconst_matrix = reconst_matrix + mu
    return np.array(reconst_matrix)


def squared_error_loss(data, u, z):
    """ Return the squared-error-loss given the data.
    :param data: A dictionary {user_id: list, question_id: list,
    is_correct: list}
    :param u: 2D matrix
    :param z: 2D matrix
    :return: float
    """
    loss = 0
    for i, q in enumerate(data["question_id"]):
        loss += (data["is_correct"][i]
                 - np.sum(u[data["user_id"][i]] * z[q])) ** 2.
    return 0.23 * loss

def update_u_z(train_data, lr, u, z):
    """ Return the updated U and Z after applying
    stochastic gradient descent for matrix completion.

    :param train_data: A dictionary {user_id: list, question_id: list,
    is_correct: list}
    :param lr: float
    :param u: 2D matrix
    :param z: 2D matrix
    :return: (u, z)
    """
    #####################################################################
    # TODO:                                                             #
    # Implement the function as described in the docstring.             #
    #####################################################################
    # Randomly select a pair (user_id, question_id).
    i = np.random.choice(len(train_data["question_id"]), 1)[0]

    c = train_data["is_correct"][i]
    n = train_data["user_id"][i]
    q = train_data["question_id"][i]

    error = c - np.dot(u[n], z[q])
    u[n] += lr * error * z[q]
    z[q] += lr * error * u[n]
    #####################################################################
    #                       END OF YOUR CODE                            #
    #####################################################################
    return u, z


def als(train_data, k, lr, num_iteration):
    """ Performs ALS algorithm, here we use the iterative solution - SGD
    rather than the direct solution.

    :param train_data: A dictionary {user_id: list, question_id: list,
    is_correct: list}
    :param k: int
    :param lr: float
    :param num_iteration: int
    :return: 2D reconstructed Matrix.
    """
    # Initialize u and z
    u = np.random.uniform(low=0, high=1 / np.sqrt(k),
                          size=(len(set(train_data["user_id"])), k))
    z = np.random.uniform(low=0, high=1 / np.sqrt(k),
                          size=(len(set(train_data["question_id"])), k))

    #####################################################################
    # TODO:                                                             #
    # Implement the function as described in the docstring.             #
    #####################################################################
    mat = None
    #####################################################################
    #                       END OF YOUR CODE                            #
    #####################################################################

    for _ in range(num_iteration):
        u, z = update_u_z(train_data, lr, u, z)

    # Return u, z, and the reconstructed matrix
    return u, z, np.dot(u, z.T)

    return np.dot(u, z.T)
    return mat

def evaluate_accuracy(data, reconstructed_matrix):
    correct_predictions = 0
    total_predictions = len(data['is_correct'])

    for i in range(total_predictions):
        user_id = data['user_id'][i]
        question_id = data['question_id'][i]
        true_value = data['is_correct'][i]

        # Forecast
        predicted_value = reconstructed_matrix[user_id, question_id] >= 0.5

        if predicted_value == true_value:
            correct_predictions += 1

    return correct_predictions / total_predictions

def main():
    # Get the absolute path to the 'sample_data' directory
    data_dir = os.path.join(os.getcwd(), 'sample_data')

    # Load data using the correct path
    train_matrix = load_train_sparse(data_dir).toarray()
    train_data = load_train_csv(data_dir)
    val_data = load_valid_csv(data_dir)
    test_data = load_public_test_csv(data_dir)

    # List of k values ​​to try
    k_values = [5, 10, 20, 50, 100]

    #SVD
    best_k_svd = None
    best_val_accuracy = 0

    # Track the test accuracy for best k on validation
    for k in k_values:
        reconstructed_matrix = svd_reconstruct(train_matrix, k)

        # Calculate accuracy on validation set
        val_accuracy = evaluate_accuracy(val_data, reconstructed_matrix)

        # Calculate accuracy on test set
        test_accuracy = evaluate_accuracy(test_data, reconstructed_matrix)

        print(f"SVD k={k} with validation accuracy: {val_accuracy}, test accuracy: {test_accuracy}")

        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            best_k_svd = k

    # Calculate test accuracy using best_k_svd
    best_svd_reconstructed_matrix = svd_reconstruct(train_matrix, best_k_svd)
    test_accuracy = evaluate_accuracy(test_data, best_svd_reconstructed_matrix)

    print(f"SVD Best k: {best_k_svd} with validation accuracy: {best_val_accuracy}, test accuracy: {test_accuracy}")

    # ALS
    num_iterations = 100
    lr = 0.01

    # Track training and validation loss for each iteration
    for k in k_values:
        training_losses = []
        validation_losses = []

        # New iteration scheme
        iteration_percentages = [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]

        for percentage in iteration_percentages:
            iterations = int((percentage * num_iterations) / 100)  # Calculate iterations for the current percentage
            u, z, reconstructed_matrix_als = als(train_data, k, lr, iterations)  # Get u, z
            train_loss = squared_error_loss(train_data, u, z)  # Pass u, z
            val_loss = squared_error_loss(val_data, u, z)  # Pass u, z

            training_losses.append(train_loss)
            validation_losses.append(val_loss)

            print(f"ALS k={k}, Iteration={iterations}/100, Training Loss={train_loss}, Validation Loss={val_loss}")

        # You can save or plot training_losses and validation_losses to analyze the behavior
        # For this code, we will simply print the last values
        print(f"ALS k={k}, Final Training Loss: {training_losses[-1]}, Final Validation Loss: {validation_losses[-1]}")

if __name__ == "__main__":
    main()